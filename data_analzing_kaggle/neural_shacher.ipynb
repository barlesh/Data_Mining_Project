{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am currently super excited about t-distributed stochastic neighbor embedding (t-SNE) for visualizing high dimensional data. Especially when training neural networks it helps a lot with understanding what's going on inside the network. In this notebook I'll use t-SNE to train and visualize a neural network that predicts housing prices. Let's start with visualizing the raw data. I prepare the data (in a very lazy way) by first checking the data type in the first non-header row. I found that there a a lot of columns with strings and integers and a few with float values. For now I work only with the integer and string values. I lazily assume that all all integer values describe ordinal or interval data and all string values are categorical. To deal with the categorical data I encode it in one-hot representation.\n",
    "After data preparation I get a 2D embedding and plot the result in a scatter plot. Color encodes the houses log values from blue/low to red/high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results\n",
      "results.csv\n",
      "test.csv\n",
      "test_new.csv\n",
      "train.csv\n",
      "train_new.csv\n",
      "\n",
      "embedding int values...\n",
      "embedding string values...\n",
      "embedding int and string values...\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn import manifold\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "data_path='/home/barlesh/Projects/Data_Mining_Project/data/'\n",
    "train_file='train.csv'\n",
    "test_file='test.csv'\n",
    "train_file_new='train_new.csv'\n",
    "test_file_new='test_new.csv'\n",
    "result_path='results/'\n",
    "result_file='res.csv'\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "from subprocess import check_output\n",
    "print(check_output([\"ls\", data_path]).decode(\"utf8\"))\n",
    "\n",
    "train = pd.read_csv(data_path+train_file)\n",
    "test = pd.read_csv(data_path+test_file)\n",
    "\n",
    "X1 = train.values[:,1:-1]\n",
    "X2 = test.values[:,1:]\n",
    "\n",
    "X = np.concatenate((X1,X2),axis=0)\n",
    "y1 = train.values[:,-1]\n",
    "y1 = np.asarray([np.log(y) for y in y1])\n",
    "#-----------------------------------------\n",
    "# Preprocessing\n",
    "#-----------------------------------------\n",
    "# 1: separate different variable types\n",
    "cat_idx = []\n",
    "float_idx = []\n",
    "int_idx = []\n",
    "for c in range(X1.shape[1]):\n",
    "    if type(X1[0,c]).__name__ == 'str':\n",
    "        cat_idx.append(c)\n",
    "    if type(X1[0,c]).__name__ == 'float' and X1[1,c]==X1[1,c]:\n",
    "        float_idx.append(c)\n",
    "    if type(X1[0,c]).__name__ == 'int':\n",
    "        int_idx.append(c)\n",
    "# 2: encode string values into numbers\n",
    "for c in cat_idx:\n",
    "    uniques = list(set(X[:,c]))\n",
    "    tmp_dict = dict(zip(uniques,range(len(uniques))))\n",
    "    n_enc = np.array([tmp_dict[s] for s in X[:,c]])\n",
    "    X[:,c] = n_enc\n",
    "        \n",
    "# 3: what does an embedding of all int values look like?\n",
    "print('embedding int values...')\n",
    "plt.figure(1)\n",
    "X_int = X[:,np.array(int_idx)]\n",
    "X_int = np.float64(X_int)\n",
    "# replace nan\n",
    "X_int[X_int!=X_int] = 0\n",
    "X_int-=np.min(X_int,axis=0)\n",
    "X_int/=(.001+np.max(X_int,axis=0))\n",
    "tsne = manifold.TSNE(n_components=2,init='pca')\n",
    "Y_int = tsne.fit_transform(X_int)\n",
    "#y1-=np.nanmin(y1)\n",
    "#y1/=np.nanmax(y1)\n",
    "plt.scatter(Y_int[len(X1):,0],Y_int[len(X1):,1],marker='.',label='test')\n",
    "sp = plt.scatter(Y_int[:len(X1),0],Y_int[:len(X1),1],c=y1,label='train')\n",
    "plt.legend(prop={'size':6})\n",
    "plt.colorbar(sp)\n",
    "plt.title('t-SNE embedding of int variables')\n",
    "plt.savefig('t-SNE_int.png')\n",
    "\n",
    "# 4: what does an embedding of all string values look like?\n",
    "print('embedding string values...')\n",
    "plt.figure(2)\n",
    "X_str = X[:,np.array(cat_idx)]\n",
    "# replace nan\n",
    "X_str[X_str!=X_str] = 0\n",
    "\n",
    "def onehot(x):\n",
    "    nx=np.zeros((len(x),max(x)+1))\n",
    "    for k in range(len(x)):\n",
    "        nx[k,x[k]] = 1\n",
    "    return nx\n",
    "\n",
    "X_tmp = []\n",
    "for c in range(X_str.shape[1]):\n",
    "    X_tmp.extend(onehot(X_str[:,c]).T)\n",
    "X_str = np.asarray(X_tmp).T\n",
    "tsne = manifold.TSNE(n_components=2,init='pca')\n",
    "Y_str = tsne.fit_transform(X_str)\n",
    "#y1-=np.nanmin(y1)\n",
    "#y1/=np.nanmax(y1)\n",
    "plt.scatter(Y_str[len(X1):,0],Y_str[len(X1):,1],marker='.',label='test')\n",
    "sp = plt.scatter(Y_str[:len(X1),0],Y_str[:len(X1),1],c=y1,label='train')\n",
    "plt.legend(prop={'size':6})\n",
    "plt.colorbar(sp)\n",
    "plt.title('t-SNE embedding of string variables')\n",
    "plt.savefig('t-SNE_string.png')\n",
    "\n",
    "# 4: what does an embedding of all int and string values look like?\n",
    "print('embedding int and string values...')\n",
    "plt.figure(3)\n",
    "X_strint = np.concatenate((X_int,X_str),axis=1)\n",
    "tsne = manifold.TSNE(n_components=2,init='pca')\n",
    "Y_strint = tsne.fit_transform(X_strint)\n",
    "plt.scatter(Y_strint[len(X1):,0],Y_strint[len(X1):,1],marker='.',label='test')\n",
    "sp = plt.scatter(Y_strint[:len(X1),0],Y_strint[:len(X1),1],c=y1,label='train')\n",
    "plt.legend(prop={'size':6})\n",
    "plt.colorbar(sp)\n",
    "plt.title('t-SNE embedding of int and string variables')\n",
    "plt.savefig('t-SNE_intstring.png')\n",
    "\n",
    "# center data at 0 scaled from -0.5 to +0.5 for neural networks\n",
    "# -> start within the linear region of tanh activation function\n",
    "X_strint-=.5\n",
    "X_strint_train = X_strint[:len(X1),:]\n",
    "X_strint_test = X_strint[len(X1):,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how a neural network changes this result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-2a73d57913dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mBatchNormalization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mregularizers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#--------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input,Dense,Dropout,BatchNormalization\n",
    "from keras import regularizers\n",
    "\n",
    "#--------------------------------------\n",
    "# Neural Network\n",
    "#--------------------------------------\n",
    "inp = Input(shape=(X_strint.shape[1],))\n",
    "D1 = Dropout(.1)(inp)\n",
    "L1_normal = Dense(64, init='uniform', activation='tanh')(D1)\n",
    "L1_sparse = Dense(1024, init='uniform', activation='tanh',activity_regularizer=regularizers.activity_l1(.001))(D1)\n",
    "L2_normal = Dense(1, init='uniform', activation='tanh')(L1_normal)\n",
    "L2_sparse = Dense(1, init='uniform', activation='tanh')(L1_sparse)\n",
    "# models that are trained to predict prices\n",
    "NN1 = Model(inp,L2_normal)\n",
    "NN2 = Model(inp,L2_sparse)\n",
    "# models for reading out activations in hidden layers\n",
    "enc1 = Model(inp,L1_normal)\n",
    "enc2 = Model(inp,L1_sparse)\n",
    "# compile models\n",
    "NN1.compile(loss='mse', optimizer='adam', metrics=['mae'])\n",
    "NN2.compile(loss='mse', optimizer='adam', metrics=['mae'])\n",
    "enc1.compile(loss='mse', optimizer='adam', metrics=['mae'])\n",
    "enc2.compile(loss='mse', optimizer='adam', metrics=['mae'])\n",
    "# train\n",
    "min_y1 = np.min(y1)\n",
    "max_y1 = np.max(y1)\n",
    "scaled_y1 = y1-min_y1\n",
    "scaled_y1/=max_y1\n",
    "scaled_y1-=.5\n",
    "NN1.fit(X_strint_train[:-50,:], scaled_y1[:-50], nb_epoch=15, batch_size=3, shuffle=True,verbose=False)\n",
    "NN2.fit(X_strint_train[:-50,:], scaled_y1[:-50], nb_epoch=15, batch_size=3, shuffle=True,verbose=False)\n",
    "\n",
    "# get hidden layer activations\n",
    "P1 = enc1.predict(X_strint)\n",
    "P2 = enc2.predict(X_strint)\n",
    "# get 2d embeddings\n",
    "tsne = manifold.TSNE(n_components=2,init='pca')\n",
    "P1_tsne = tsne.fit_transform(P1)\n",
    "P2_tsne = tsne.fit_transform(P2)\n",
    "\n",
    "P1_tsne_train = P1_tsne[:len(X1),:]\n",
    "P2_tsne_train = P2_tsne[:len(X1),:]\n",
    "P1_tsne_test = P1_tsne[len(X1):,:]\n",
    "P2_tsne_test = P2_tsne[len(X1):,:]\n",
    "\n",
    "print(P1_tsne_train)\n",
    "print(P1_tsne_test)\n",
    "\n",
    "plt.figure(1)\n",
    "plt.scatter(P1_tsne_test[:,0],P1_tsne_test[:,1],marker='.',label='test')\n",
    "sp1 = plt.scatter(P1_tsne_train[:-50,0],P1_tsne_train[:-50,1],c=y1[:-50],label='train')\n",
    "plt.scatter(P1_tsne_train[-50:,0],P1_tsne_train[-50:,1],marker='^',s=55, c=y1[-50:],label='validation')\n",
    "plt.colorbar(sp1)\n",
    "plt.legend(prop={'size':6})\n",
    "plt.title('t-SNE embedding layer1')\n",
    "plt.savefig('t-SNE_layer1.png')\n",
    "\n",
    "plt.figure(2)\n",
    "plt.scatter(P2_tsne_test[:,0],P2_tsne_test[:,1],marker='.',label='test')\n",
    "sp2 = plt.scatter(P2_tsne_train[:-50,0],P2_tsne_train[:-50,1],c=y1[:-50],label='train')\n",
    "plt.scatter(P2_tsne_train[-50:,0],P2_tsne_train[-50:,1],marker='^',s=55, c=y1[-50:],label='validation')\n",
    "plt.colorbar(sp2)\n",
    "plt.legend(prop={'size':6})\n",
    "plt.title('t-SNE embedding sparse layer1')\n",
    "plt.savefig('t-SNE_layer1sparse.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I trained two different neural networks, each with one hidden layer: \n",
    "Network 1 has 64 units in its hidden layer, i.e. the data was compressed to 64 dimensions.\n",
    "Network 2 has a sparse hidden layer with 1024 units.\n",
    "\n",
    "While Network 1 did a better job in terms of data representation in the shown examples, this does not seem to be the general case as I did observe opposite results in different runs.\n",
    "\n",
    "However what I could observe over multiple runs is that the hidden layers of both networks rearrange the data so that the distance between data points better represents their difference in value.\n",
    "Let's see what happens if we add more layers to the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#--------------------------------------\n",
    "# Deep Neural Network\n",
    "#--------------------------------------\n",
    "inp = Input(shape=(X_strint.shape[1],))\n",
    "D1 = Dropout(.1)(inp)\n",
    "L1 = Dense(64, init='uniform', activation='tanh')(D1)\n",
    "N1 = BatchNormalization()(L1)\n",
    "D2 = Dropout(.2)(N1)\n",
    "L2 = Dense(64, init='uniform', activation='tanh')(D2)\n",
    "N2 = BatchNormalization()(L2)\n",
    "D3 = Dropout(.2)(N2)\n",
    "L3 = Dense(36, init='uniform', activation='tanh')(D3)\n",
    "N3 = BatchNormalization()(L3)\n",
    "D4 = Dropout(.2)(N3)\n",
    "L4 = Dense(1, init='uniform', activation='tanh')(D4)\n",
    "# model that is trained to predict prices\n",
    "model1 = Model(inp,L4)\n",
    "# models for reading out activations in hidden layers\n",
    "enc_l1 = Model(inp,L1)\n",
    "enc_l2 = Model(inp,L2)\n",
    "enc_l3 = Model(inp,L3)\n",
    "# compile models\n",
    "model1.compile(loss='mse', optimizer='adam', metrics=['mae'])\n",
    "enc_l1.compile(loss='mse', optimizer='adam', metrics=['mae'])\n",
    "enc_l2.compile(loss='mse', optimizer='adam', metrics=['mae'])\n",
    "enc_l3.compile(loss='mse', optimizer='adam', metrics=['mae'])\n",
    "# train\n",
    "min_y1 = np.min(y1)\n",
    "max_y1 = np.max(y1)\n",
    "scaled_y1 = y1-min_y1\n",
    "scaled_y1/=max_y1\n",
    "model1.fit(X_strint_train[:-50,:], scaled_y1[:-50], nb_epoch=55, batch_size=3, shuffle=True,verbose=False)\n",
    "# get hidden layer activations\n",
    "P1 = enc_l1.predict(X_strint)\n",
    "P2 = enc_l2.predict(X_strint)\n",
    "P3 = enc_l3.predict(X_strint)\n",
    "# get 2d embeddings\n",
    "tsne = manifold.TSNE(n_components=2,init='pca')\n",
    "P1_tsne = tsne.fit_transform(P1)\n",
    "P2_tsne = tsne.fit_transform(P2)\n",
    "P3_tsne = tsne.fit_transform(P3)\n",
    "\n",
    "P1_tsne_train = P1_tsne[:len(X1),:]\n",
    "P2_tsne_train = P2_tsne[:len(X1),:]\n",
    "P3_tsne_train = P3_tsne[:len(X1),:]\n",
    "P1_tsne_test = P1_tsne[len(X1):,:]\n",
    "P2_tsne_test = P2_tsne[len(X1):,:]\n",
    "P3_tsne_test = P3_tsne[len(X1):,:]\n",
    "\n",
    "plt.figure(3)\n",
    "plt.scatter(P1_tsne_test[:,0],P1_tsne_test[:,1],marker='.',label='test')\n",
    "sp1 = plt.scatter(P1_tsne_train[:-50,0],P1_tsne_train[:-50,1],c=y1[:-50],label='train')\n",
    "plt.scatter(P1_tsne_train[-50:,0],P1_tsne_train[-50:,1],marker='^',s=55, c=y1[-50:],label='validation')\n",
    "plt.colorbar(sp1)\n",
    "plt.legend(prop={'size':6})\n",
    "plt.title('t-SNE embedding: deep network - layer1')\n",
    "plt.savefig('t-SNE_deep_layer1.png')\n",
    "\n",
    "plt.figure(4)\n",
    "plt.scatter(P2_tsne_test[:,0],P2_tsne_test[:,1],marker='.',label='test')\n",
    "sp1 = plt.scatter(P2_tsne_train[:-50,0],P2_tsne_train[:-50,1],c=y1[:-50],label='train')\n",
    "plt.scatter(P2_tsne_train[-50:,0],P2_tsne_train[-50:,1],marker='^',s=55, c=y1[-50:],label='validation')\n",
    "plt.colorbar(sp1)\n",
    "plt.legend(prop={'size':6})\n",
    "plt.title('t-SNE embedding: deep network - layer2')\n",
    "plt.savefig('t-SNE_deep_layer2.png')\n",
    "\n",
    "plt.figure(5)\n",
    "plt.scatter(P3_tsne_test[:,0],P3_tsne_test[:,1],marker='.',label='test')\n",
    "sp1 = plt.scatter(P3_tsne_train[:-50,0],P3_tsne_train[:-50,1],c=y1[:-50],label='train')\n",
    "plt.scatter(P3_tsne_train[-50:,0],P3_tsne_train[-50:,1],marker='^',s=55, c=y1[-50:],label='validation')\n",
    "plt.colorbar(sp1)\n",
    "plt.legend(prop={'size':6})\n",
    "plt.title('t-SNE embedding: deep network - layer3')\n",
    "plt.savefig('t-SNE_deep_layer3.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
