{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The idea is:**\n",
    "\n",
    " - Feature reduction with PCA\n",
    " - Data transformation (log, hot encoding, nan)\n",
    " - Test different regression models\n",
    "\n",
    "**Things found:**\n",
    "\n",
    "- Applying log transformation really increases the accuracy.\n",
    "- Using PCA with 36 components makes the learning and testing much (much much) faster.\n",
    "- Removing columns with more than 1000 NaNs gives better result than applying \"mean\" to them.\n",
    "- There are outliers. Instead of removing them, using Huber seems to provide a good result. Huber is a model robust to outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "#import tflearn\n",
    "#import tensorflow as tf\n",
    "import seaborn\n",
    "import warnings\n",
    "import subprocess\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from subprocess import check_output\n",
    "#subprocess.check_output([\"ls\",\"C:/Users/yuvalanavim/Desktop/test_regrission\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path='/home/barlesh/Projects/Data_Mining_Project/data/'\n",
    "train_file='train.csv'\n",
    "test_file='test.csv'\n",
    "train_file_new='train_new.csv'\n",
    "test_file_new='test_new.csv'\n",
    "result_path='results/'\n",
    "result_file='res.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Load ##\n",
    "\n",
    "I mix data and test to manipulate all the data just once. SalePrice is extracted to its own variable \"labels\". Finally, SalesPrice is remove from data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(data_path+train_file_new)\n",
    "labels=train[\"SalePrice\"]\n",
    "test = pd.read_csv(data_path+test_file_new)\n",
    "data = pd.concat([train,test],ignore_index=True)\n",
    "data = data.drop(\"SalePrice\", 1)\n",
    "ids = test[\"Id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>0</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>0</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>0</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>0</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>0</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 82 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Id  MSSubClass MSZoning  LotFrontage  LotArea Street  Alley  \\\n",
       "0           0   1          60       RL         65.0     8450   Pave      0   \n",
       "1           1   2          20       RL         80.0     9600   Pave      0   \n",
       "2           2   3          60       RL         68.0    11250   Pave      0   \n",
       "3           3   4          70       RL         60.0     9550   Pave      0   \n",
       "4           4   5          60       RL         84.0    14260   Pave      0   \n",
       "\n",
       "  LotShape LandContour    ...     PoolArea PoolQC Fence MiscFeature MiscVal  \\\n",
       "0      Reg         Lvl    ...            0   None  None        None       0   \n",
       "1      Reg         Lvl    ...            0   None  None        None       0   \n",
       "2      IR1         Lvl    ...            0   None  None        None       0   \n",
       "3      IR1         Lvl    ...            0   None  None        None       0   \n",
       "4      IR1         Lvl    ...            0   None  None        None       0   \n",
       "\n",
       "  MoSold YrSold SaleType  SaleCondition  SalePrice  \n",
       "0      2   2008       WD         Normal     208500  \n",
       "1      5   2007       WD         Normal     181500  \n",
       "2      9   2008       WD         Normal     223500  \n",
       "3      2   2006       WD        Abnorml     140000  \n",
       "4     12   2008       WD         Normal     250000  \n",
       "\n",
       "[5 rows x 82 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1460"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the number of rows in train\n",
    "train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2919"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the number of rows in total\n",
    "data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], dtype: int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the number of NaNs each column has.\n",
    "nans=pd.isnull(data).sum()\n",
    "nans[nans>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remove id and columns with more than a thousand missing values\n",
    "data=data.drop(\"Id\", 1)\n",
    "data=data.drop(\"Alley\", 1)\n",
    "data=data.drop(\"Fence\", 1)\n",
    "data=data.drop(\"MiscFeature\", 1)\n",
    "data=data.drop(\"PoolQC\", 1)\n",
    "data=data.drop(\"FireplaceQu\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "object     38\n",
       "int64      25\n",
       "float64    11\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the column types\n",
    "data.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Manipulation ##\n",
    "\n",
    "- Apply hot encoding, convert categorical variable into dummy/indicator variables.\n",
    "- Fill NaN with median for that column.\n",
    "- Log transformation.\n",
    "- Change -inf to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_columns = data.columns.values\n",
    "non_categorical = [\"LotFrontage\", \"LotArea\", \"MasVnrArea\", \"BsmtFinSF1\", \n",
    "                   \"BsmtFinSF2\", \"BsmtUnfSF\", \"TotalBsmtSF\", \"1stFlrSF\", \n",
    "                   \"2ndFlrSF\", \"LowQualFinSF\", \"GrLivArea\", \"GarageArea\", \n",
    "                   \"WoodDeckSF\", \"OpenPorchSF\", \"EnclosedPorch\", \"3SsnPorch\", \n",
    "                   \"ScreenPorch\",\"PoolArea\", \"MiscVal\"]\n",
    "\n",
    "categorical = [value for value in all_columns if value not in non_categorical]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# One Hot Encoding and nan transformation\n",
    "data = pd.get_dummies(data)\n",
    "\n",
    "imp = Imputer(missing_values='NaN', strategy='most_frequent', axis=0)\n",
    "data = imp.fit_transform(data)\n",
    "\n",
    "# Log transformation\n",
    "data = np.log(data)\n",
    "labels = np.log(labels)\n",
    "\n",
    "# Change -inf to 0 again\n",
    "data[data==-np.inf]=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature reduction ##\n",
    "\n",
    "There are many features, so I am going to use PCA to reduce them. The idea is to start with n_components = number of columns. Then select the number of components that add up to 1 variance_ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.2248857 ,  0.40281429,  0.52425789,  0.62418823,  0.69580422,\n",
       "        0.75944463,  0.8116806 ,  0.85647038,  0.89178708,  0.92273755,\n",
       "        0.94898868,  0.95842727,  0.96637545,  0.97380464,  0.97971901,\n",
       "        0.98501952,  0.98918839,  0.99199181,  0.99386559,  0.99520919,\n",
       "        0.99611479,  0.99695667,  0.99771023,  0.99842564,  0.9989402 ,\n",
       "        0.99933882,  0.99959949,  0.99978254,  0.99988174,  0.99993998,\n",
       "        0.99998599,  0.99999658,  0.99999871,  0.99999943,  0.99999999,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = PCA(whiten=True)\n",
    "pca.fit(data)\n",
    "variance = pd.DataFrame(pca.explained_variance_ratio_)\n",
    "np.cumsum(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=36,whiten=True)\n",
    "pca = pca.fit(data)\n",
    "dataPCA = pca.transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Model Selection ##\n",
    "\n",
    "Simple test to run multiple models against our data. First, with raw features. No PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split traing and test\n",
    "train = data[:1460]\n",
    "test = data[1460:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_squared_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AdaBoost</th>\n",
       "      <td>0.032080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM RBF</th>\n",
       "      <td>0.029442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForest</th>\n",
       "      <td>0.026025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hubber</th>\n",
       "      <td>0.025425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ridge</th>\n",
       "      <td>0.025091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bagging</th>\n",
       "      <td>0.024431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM Linear</th>\n",
       "      <td>0.023461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bayesian Ridge</th>\n",
       "      <td>0.022895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Linear</th>\n",
       "      <td>0.022695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lasso</th>\n",
       "      <td>0.022695</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                mean_squared_error\n",
       "AdaBoost                  0.032080\n",
       "SVM RBF                   0.029442\n",
       "RandomForest              0.026025\n",
       "Hubber                    0.025425\n",
       "Ridge                     0.025091\n",
       "Bagging                   0.024431\n",
       "SVM Linear                0.023461\n",
       "Bayesian Ridge            0.022895\n",
       "Linear                    0.022695\n",
       "Lasso                     0.022695"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# R2 Score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "def lets_try(train,labels):\n",
    "    results={}\n",
    "    def test_model(clf):\n",
    "        \n",
    "        cv = KFold(n_splits=5,shuffle=True,random_state=45)\n",
    "        mse0 = make_scorer(mean_squared_error)\n",
    "        mse0_val_score = cross_val_score(clf, train, labels, cv=cv,scoring=mse0)\n",
    "        scores=[mse0_val_score.mean()]\n",
    "        return scores\n",
    "\n",
    "    clf = linear_model.LinearRegression()\n",
    "    results[\"Linear\"]=test_model(clf)\n",
    "    \n",
    "    clf = linear_model.Ridge()\n",
    "    results[\"Ridge\"]=test_model(clf)\n",
    "    \n",
    "    clf = linear_model.BayesianRidge()\n",
    "    results[\"Bayesian Ridge\"]=test_model(clf)\n",
    "    \n",
    "    clf = linear_model.HuberRegressor()\n",
    "    results[\"Hubber\"]=test_model(clf)\n",
    "    \n",
    "    clf = linear_model.Lasso(alpha=1e-4)\n",
    "    results[\"Lasso\"]=test_model(clf)\n",
    "    \n",
    "    clf = BaggingRegressor()\n",
    "    results[\"Bagging\"]=test_model(clf)\n",
    "    \n",
    "    clf = RandomForestRegressor()\n",
    "    results[\"RandomForest\"]=test_model(clf)\n",
    "    \n",
    "    clf = AdaBoostRegressor()\n",
    "    results[\"AdaBoost\"]=test_model(clf)\n",
    "    \n",
    "    clf = svm.SVR()\n",
    "    results[\"SVM RBF\"]=test_model(clf)\n",
    "    \n",
    "    clf = svm.SVR(kernel=\"linear\")\n",
    "    results[\"SVM Linear\"]=test_model(clf)\n",
    "    \n",
    "    results = pd.DataFrame.from_dict(results,orient='index')\n",
    "    results.columns=[\"mean_squared_error\"] \n",
    "    results=results.sort(columns=[\"mean_squared_error\"],ascending=False)\n",
    "    results.plot(kind=\"bar\",title=\"Model Scores\")\n",
    "    axes = plt.gca()\n",
    "    axes.set_ylim([0.5,1])\n",
    "    return results\n",
    "\n",
    "lets_try(train,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try the same but using data with PCA applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_squared_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AdaBoost</th>\n",
       "      <td>0.052733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bagging</th>\n",
       "      <td>0.046139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForest</th>\n",
       "      <td>0.043924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM RBF</th>\n",
       "      <td>0.033351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Linear</th>\n",
       "      <td>0.022695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ridge</th>\n",
       "      <td>0.022691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lasso</th>\n",
       "      <td>0.022684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bayesian Ridge</th>\n",
       "      <td>0.022683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM Linear</th>\n",
       "      <td>0.021980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hubber</th>\n",
       "      <td>0.021777</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                mean_squared_error\n",
       "AdaBoost                  0.052733\n",
       "Bagging                   0.046139\n",
       "RandomForest              0.043924\n",
       "SVM RBF                   0.033351\n",
       "Linear                    0.022695\n",
       "Ridge                     0.022691\n",
       "Lasso                     0.022684\n",
       "Bayesian Ridge            0.022683\n",
       "SVM Linear                0.021980\n",
       "Hubber                    0.021777"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split traing and test\n",
    "train = dataPCA[:1460]\n",
    "test = dataPCA[1460:]\n",
    "\n",
    "lets_try(train,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuberRegressor(alpha=100, epsilon=1.25, fit_intercept=True, max_iter=100,\n",
       "        tol=1e-10, warm_start=False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = KFold(n_splits=5,shuffle=True,random_state=45)\n",
    "\n",
    "parameters = {'alpha': [1000,200,100,10],\n",
    "              'epsilon' : [1.2,1.25,1.50,1.7],\n",
    "              'tol' : [1e-10]}\n",
    "\n",
    "clf = linear_model.HuberRegressor()\n",
    "r2 = make_scorer(r2_score)\n",
    "grid_obj = GridSearchCV(clf, parameters, cv=cv,scoring=r2)\n",
    "grid_fit = grid_obj.fit(train, labels)\n",
    "best_clf = grid_fit.best_estimator_ \n",
    "\n",
    "best_clf.fit(train,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.01816323  0.01446544  0.01732366  0.03132449  0.02611438  0.01709237\n",
      "  0.01553854  0.01677424  0.03745242  0.01977055]\n",
      "0.0214019324452\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse1 = make_scorer(mean_squared_error)\n",
    "scores = cross_val_score(clf, train, labels, cv=10, scoring=mse1)\n",
    "print(scores)\n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Id      SalePrice\n",
      "0     1461  125634.332715\n",
      "1     1462  156328.599906\n",
      "2     1463  188741.597383\n",
      "3     1464  205770.311280\n",
      "4     1465  189372.909898\n",
      "5     1466  178681.501287\n",
      "6     1467  179472.043456\n",
      "7     1468  165370.798502\n",
      "8     1469  197880.321555\n",
      "9     1470  112413.323545\n",
      "10    1471  194284.990891\n",
      "11    1472  100752.687283\n",
      "12    1473   97447.400774\n",
      "13    1474  145476.638901\n",
      "14    1475  107890.142830\n",
      "15    1476  298584.983669\n",
      "16    1477  237563.835783\n",
      "17    1478  273118.370511\n",
      "18    1479  266711.924184\n",
      "19    1480  397868.372156\n",
      "20    1481  295304.416582\n",
      "21    1482  211222.230065\n",
      "22    1483  190166.235252\n",
      "23    1484  164809.067137\n",
      "24    1485  194596.520682\n",
      "25    1486  200355.441281\n",
      "26    1487  284752.908219\n",
      "27    1488  237995.529341\n",
      "28    1489  187415.228172\n",
      "29    1490  224288.820580\n",
      "...    ...            ...\n",
      "1429  2890   73369.977831\n",
      "1430  2891  140937.919956\n",
      "1431  2892   69287.343706\n",
      "1432  2893  104920.418637\n",
      "1433  2894   69616.560743\n",
      "1434  2895  293983.782180\n",
      "1435  2896  243757.028420\n",
      "1436  2897  225760.631540\n",
      "1437  2898  155247.241083\n",
      "1438  2899  213993.576788\n",
      "1439  2900  170775.617193\n",
      "1440  2901  230348.256523\n",
      "1441  2902  195727.393297\n",
      "1442  2903  316196.048936\n",
      "1443  2904  301806.722124\n",
      "1444  2905   71412.632365\n",
      "1445  2906  212870.896457\n",
      "1446  2907  114203.071759\n",
      "1447  2908  128161.434588\n",
      "1448  2909  153689.969998\n",
      "1449  2910   75687.494784\n",
      "1450  2911   82988.904230\n",
      "1451  2912  151750.345125\n",
      "1452  2913   88308.764067\n",
      "1453  2914   79705.475809\n",
      "1454  2915   87801.474821\n",
      "1455  2916   89785.304054\n",
      "1456  2917  177293.744508\n",
      "1457  2918  124159.642900\n",
      "1458  2919  241313.869243\n",
      "\n",
      "[1459 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "predictions_huber = best_clf.predict(test)\n",
    "predictions_huber = np.exp(predictions_huber)\n",
    "sub = pd.DataFrame({\n",
    "        \"Id\": ids,\n",
    "        \"SalePrice\": predictions_huber\n",
    "    })\n",
    "sub.to_csv(\"prices_huber1.csv\", index=False)\n",
    "print(sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM tranning and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rmse_cv(model):\n",
    "    rmse= np.sqrt(-cross_val_score(model, train, labels, scoring=\"neg_mean_squared_error\", cv = 5))\n",
    "    return(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on all data: \n",
      "0.148731554646\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Training on all data: ')\n",
    "model_svm = svm.SVR(kernel=\"linear\", C=3,)\n",
    "model_svm.fit(train, labels)\n",
    "\n",
    "\n",
    "\n",
    "cv_rmse_svm = rmse_cv(model_svm).mean()\n",
    "print (cv_rmse_svm)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = model_svm.predict(test)\n",
    "predictions_svm = np.exp(predictions)\n",
    "sub = pd.DataFrame({\n",
    "        \"Id\": ids,\n",
    "        \"SalePrice\": predictions_svm\n",
    "    })\n",
    "sub.to_csv(\"prices_svm1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.02345859  0.01640437  0.02317852  0.03338032  0.029027    0.0226658\n",
      "  0.02265506  0.02255614  0.03490317  0.02274871]\n",
      "0.0250977686718\n"
     ]
    }
   ],
   "source": [
    "mse1 = make_scorer(mean_squared_error)\n",
    "scores = cross_val_score(clf, train, labels, cv=10, scoring=mse1)\n",
    "print(scores)\n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Neural Network\n",
    "---------------------\n",
    "\n",
    "Now I am going to try a simple neural network, to see if i can improve the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Shape the labels\n",
    "labels_nl = labels\n",
    "labels_nl = labels_nl.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "r2 = tflearn.R2()\n",
    "net = tflearn.input_data(shape=[None, train.shape[1]])\n",
    "net = tflearn.fully_connected(net, 30, activation='linear')\n",
    "net = tflearn.fully_connected(net, 10, activation='linear')\n",
    "net = tflearn.fully_connected(net, 1, activation='linear')\n",
    "sgd = tflearn.SGD(learning_rate=0.1, lr_decay=0.01, decay_step=100)\n",
    "net = tflearn.regression(net, optimizer=sgd,loss='mean_square',metric=r2)\n",
    "model = tflearn.DNN(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit(train, labels_nl,show_metric=True,validation_set=0.2,shuffle=True,n_epoch=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "\n",
    "predictions_huber = best_clf.predict(test)\n",
    "predictions_DNN = model.predict(test)\n",
    "predictions_huber = np.exp(predictions_huber)\n",
    "predictions_DNN = np.exp(predictions_DNN)\n",
    "predictions_DNN = predictions_DNN.reshape(-1,)\n",
    "\n",
    "sub = pd.DataFrame({\n",
    "        \"Id\": ids,\n",
    "        \"SalePrice\": predictions_DNN\n",
    "    })\n",
    "\n",
    "sub.to_csv(\"prices_submission.csv\", index=False)\n",
    "#print(sub)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
